{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b5d20c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "# Render the figure in a notebook:\n",
    "%matplotlib inline  \n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchmetrics.classification import Accuracy\n",
    "from torchmetrics.classification import Recall\n",
    "from torchmetrics import R2Score\n",
    "from torchmetrics import MeanAbsoluteError\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ddb68b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('../no_nontoxic_nans_X_train.csv',index_col=0)\n",
    "y_train = pd.read_csv('../no_nontoxic_nans_y_train.csv',index_col=0)\n",
    "\n",
    "x_val = pd.read_csv('../no_nontoxic_nans_x_val.csv')\n",
    "y_val = pd.read_csv('../no_nontoxic_nans_y_val.csv',index_col=0)\n",
    "\n",
    "x_test = pd.read_csv('../no_nontoxic_nans_x_test.csv')\n",
    "y_test = pd.read_csv('../no_nontoxic_nans_y_test.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f1ca84",
   "metadata": {},
   "source": [
    "### You have been using DataFrames because these are easy to deal with but here we need the data as an array\n",
    "need to make a numpy array, followed by a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "53ae833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads all the data into special pytorch data structures.\n",
    "\n",
    "X_train_array = x_train.to_numpy()\n",
    "X_train_tensor = torch.Tensor(X_train_array)\n",
    "\n",
    "X_val_array = x_val.to_numpy()\n",
    "X_val_tensor = torch.Tensor(X_val_array)\n",
    "\n",
    "y_train_array = y_train.to_numpy()\n",
    "y_train_tensor = torch.Tensor(y_train_array)\n",
    "\n",
    "y_val_array = y_val.to_numpy()\n",
    "y_val_tensor = torch.Tensor(y_val_array)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "trainloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "valloader = DataLoader(val_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "32bc9aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7823,  0.7823, -0.4119,  ..., -0.0907, -0.2697, -0.1562],\n",
       "        [ 0.5145,  0.5145, -0.7155,  ..., -0.0907, -0.2697, -0.1562],\n",
       "        [-1.5689, -1.5689,  0.6186,  ..., -0.0907, -0.2697, -0.1562],\n",
       "        ...,\n",
       "        [ 0.2563,  0.2563, -0.5645,  ..., -0.0907, -0.2697, -0.1562],\n",
       "        [ 0.2330,  0.2330, -0.1187,  ..., -0.0907, -0.2697, -0.1562],\n",
       "        [ 0.7342,  0.7342, -0.7105,  ..., -0.0907, -0.2697, -0.1562]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ef3d695b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]], shape=(3691, 1))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a0c2133a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1,  ..., 0, 1, 1])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor = y_train_tensor.squeeze()  # Converts shape (batch_size, 1) to (batch_size,)\n",
    "y_val_tensor = y_val_tensor.squeeze()      # Converts shape (batch_size, 1) to (batch_size,)\n",
    "\n",
    "# Ensure the target tensor is of type long (int64)\n",
    "y_train_tensor = y_train_tensor.long()\n",
    "y_val_tensor = y_val_tensor.long()\n",
    "y_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "68b2f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### effectively we are making something like LinearRegression ourselves\n",
    "class MyPerceptron(nn.Module):    # this class inherits from nn.Module\n",
    "    def __init__(self):\n",
    "        super(MyPerceptron, self).__init__() #this calls the constructor of the parent class nn.Module\n",
    "        \n",
    "        # define network layers\n",
    "        self.fc1 = nn.Linear(217, 1)   # nn.Linear is a class for linear layers, i.e. input to the first hidden layer\n",
    "        self.fc2 = nn.Linear(1, 2) # this is the hidden layer to the output layer with two outputs (0 or 1)\n",
    "        \n",
    "        # Here we are using a linear model as the activation function\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1b69a3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss = nan\n",
      "Epoch 2 Loss = nan\n",
      "Epoch 3 Loss = nan\n",
      "Epoch 4 Loss = nan\n",
      "Epoch 5 Loss = nan\n",
      "Epoch 6 Loss = nan\n",
      "Epoch 7 Loss = nan\n",
      "Epoch 8 Loss = nan\n",
      "Epoch 9 Loss = nan\n",
      "Epoch 10 Loss = nan\n",
      "Epoch 11 Loss = nan\n",
      "Epoch 12 Loss = nan\n",
      "Epoch 13 Loss = nan\n",
      "Epoch 14 Loss = nan\n",
      "Epoch 15 Loss = nan\n",
      "Epoch 16 Loss = nan\n",
      "Epoch 17 Loss = nan\n",
      "Epoch 18 Loss = nan\n",
      "Epoch 19 Loss = nan\n",
      "Epoch 20 Loss = nan\n",
      "Epoch 21 Loss = nan\n",
      "Epoch 22 Loss = nan\n",
      "Epoch 23 Loss = nan\n",
      "Epoch 24 Loss = nan\n",
      "Epoch 25 Loss = nan\n",
      "Epoch 26 Loss = nan\n",
      "Epoch 27 Loss = nan\n",
      "Epoch 28 Loss = nan\n",
      "Epoch 29 Loss = nan\n",
      "Epoch 30 Loss = nan\n",
      "Epoch 31 Loss = nan\n",
      "Epoch 32 Loss = nan\n",
      "Epoch 33 Loss = nan\n",
      "Epoch 34 Loss = nan\n",
      "Epoch 35 Loss = nan\n",
      "Epoch 36 Loss = nan\n",
      "Epoch 37 Loss = nan\n",
      "Epoch 38 Loss = nan\n",
      "Epoch 39 Loss = nan\n",
      "Epoch 40 Loss = nan\n",
      "Epoch 41 Loss = nan\n",
      "Epoch 42 Loss = nan\n",
      "Epoch 43 Loss = nan\n",
      "Epoch 44 Loss = nan\n",
      "Epoch 45 Loss = nan\n",
      "Epoch 46 Loss = nan\n",
      "Epoch 47 Loss = nan\n",
      "Epoch 48 Loss = nan\n",
      "Epoch 49 Loss = nan\n",
      "Epoch 50 Loss = nan\n",
      "Epoch 51 Loss = nan\n",
      "Epoch 52 Loss = nan\n",
      "Epoch 53 Loss = nan\n",
      "Epoch 54 Loss = nan\n",
      "Epoch 55 Loss = nan\n",
      "Epoch 56 Loss = nan\n",
      "Epoch 57 Loss = nan\n",
      "Epoch 58 Loss = nan\n",
      "Epoch 59 Loss = nan\n",
      "Epoch 60 Loss = nan\n",
      "Epoch 61 Loss = nan\n",
      "Epoch 62 Loss = nan\n",
      "Epoch 63 Loss = nan\n",
      "Epoch 64 Loss = nan\n",
      "Epoch 65 Loss = nan\n",
      "Epoch 66 Loss = nan\n",
      "Epoch 67 Loss = nan\n",
      "Epoch 68 Loss = nan\n",
      "Epoch 69 Loss = nan\n",
      "Epoch 70 Loss = nan\n",
      "Epoch 71 Loss = nan\n",
      "Epoch 72 Loss = nan\n",
      "Epoch 73 Loss = nan\n",
      "Epoch 74 Loss = nan\n",
      "Epoch 75 Loss = nan\n",
      "Epoch 76 Loss = nan\n",
      "Epoch 77 Loss = nan\n",
      "Epoch 78 Loss = nan\n",
      "Epoch 79 Loss = nan\n",
      "Epoch 80 Loss = nan\n",
      "Epoch 81 Loss = nan\n",
      "Epoch 82 Loss = nan\n",
      "Epoch 83 Loss = nan\n",
      "Epoch 84 Loss = nan\n",
      "Epoch 85 Loss = nan\n",
      "Epoch 86 Loss = nan\n",
      "Epoch 87 Loss = nan\n",
      "Epoch 88 Loss = nan\n",
      "Epoch 89 Loss = nan\n",
      "Epoch 90 Loss = nan\n",
      "Epoch 91 Loss = nan\n",
      "Epoch 92 Loss = nan\n",
      "Epoch 93 Loss = nan\n",
      "Epoch 94 Loss = nan\n",
      "Epoch 95 Loss = nan\n",
      "Epoch 96 Loss = nan\n",
      "Epoch 97 Loss = nan\n",
      "Epoch 98 Loss = nan\n",
      "Epoch 99 Loss = nan\n",
      "Epoch 100 Loss = nan\n"
     ]
    }
   ],
   "source": [
    "### set a random seed ###\n",
    "torch.manual_seed(0)\n",
    "\n",
    "### create the network set the criretia and how to optimize\n",
    "net1 = MyPerceptron()\n",
    "net1.train() ### this turns the model on for training (above there are no specific layers only involved in training but this is good practise)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net1.parameters(), lr=0.0001)\n",
    "\n",
    "### set the number of epochs to run\n",
    "num_epochs = 100  \n",
    "\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data ### extacts the features and ground truth labels\n",
    "        optimizer.zero_grad() ### removes gradients from previous run in backpropergation\n",
    "        outputs = net1(inputs) ### gets the predictions - which is now the raw scores \"logits\"\n",
    "        labels = labels.squeeze().long()\n",
    "        if inputs.size(0) < 128:\n",
    "            continue  \n",
    "        # print(f\"Batch {i+1}: Inputs shape = {inputs.shape}, Labels shape = {labels.shape}\")\n",
    "        loss = criterion(outputs, labels.long()) ### calculates the loss of the outputs with the labels\n",
    "        loss.backward() ### computes the gradients for backpropagation\n",
    "        optimizer.step() ### updates the model based on the gradients\n",
    "        running_loss += loss.item() ### adds the loss to the running_loss \n",
    "\n",
    "    avg_train_loss = running_loss / len(trainloader) ### calculates the average lss for the batches\n",
    "\n",
    "    print('Epoch',epoch,'Loss =',avg_train_loss) ### prints the info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2a78964d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 1, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 2, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 3, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 4, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 5, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 6, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 7, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 8, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 9, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 10, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 11, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 12, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 13, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 14, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 15, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 16, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 17, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 18, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 19, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 20, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 21, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 22, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 23, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 24, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 25, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 26, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 27, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 28, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 29, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 30, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 31, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 32, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 33, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 34, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 35, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 36, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 37, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 38, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 39, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 40, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 41, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 42, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 43, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 44, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 45, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 46, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 47, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 48, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 49, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 50, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 51, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 52, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 53, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 54, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 55, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 56, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 57, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 58, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 59, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 60, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 61, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 62, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 63, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 64, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 65, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 66, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 67, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 68, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 69, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 70, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 71, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 72, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 73, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 74, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 75, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 76, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 77, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 78, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 79, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 80, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 81, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 82, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 83, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 84, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 85, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 86, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 87, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 88, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 89, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 90, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 91, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 92, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 93, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 94, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 95, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 96, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 97, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 98, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 99, Loss = 0.0000\n",
      "NaN detected in outputs at batch 1\n",
      "NaN detected in loss at batch 1\n",
      "Epoch 100, Loss = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Skip small batches (e.g., if batch size is less than 128)\n",
    "        if inputs.size(0) < 128:\n",
    "            print(f\"Skipping batch {i+1} due to small size ({inputs.size(0)} samples)\")\n",
    "            continue\n",
    "        \n",
    "        labels = labels.squeeze().long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net1(inputs)\n",
    "\n",
    "        # Check for NaN values in outputs\n",
    "        if torch.any(torch.isnan(outputs)):\n",
    "            print(f\"NaN detected in outputs at batch {i+1}\")\n",
    "        \n",
    "        # Check for NaN values in labels\n",
    "        if torch.any(torch.isnan(labels)):\n",
    "            print(f\"NaN detected in labels at batch {i+1}\")\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Check if the loss is NaN\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"NaN detected in loss at batch {i+1}\")\n",
    "            break  # Stop if loss becomes NaN\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(net1.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(trainloader)\n",
    "    print(f\"Epoch {epoch}, Loss = {avg_train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c449bbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3691])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2b93e6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "94272f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 2])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = net1(inputs)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a9e58a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([107])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab22904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
